{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    return [1, 2, 3, 4, 5]\n",
    "\n",
    "def test_sum(sample_data):\n",
    "    assert sum(sample_data) == 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def db_connection():\n",
    "    conn = \"connected_to_db\"  # Simplified for the example\n",
    "    yield conn\n",
    "    conn = \"disconnected_from_db\"\n",
    "\n",
    "def test_db_query(db_connection):\n",
    "    assert db_connection == \"connected_to_db\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "# Parameterized Testing\n",
    "@pytest.mark.parametrize(\"input,expected\", [(2, 2), (3, 6), (4, 24)])\n",
    "def test_factorial_param(input, expected):\n",
    "    assert factorial(input) == expected\n",
    "\n",
    "# Skipping Tests\n",
    "@pytest.mark.skip(reason=\"Skipping this test as an example.\")\n",
    "def test_skip_example():\n",
    "    assert reverse_string('hello') == 'hello'\n",
    "\n",
    "# Expected Failures\n",
    "@pytest.mark.xfail\n",
    "def test_expected_failure():\n",
    "    assert factorial(0) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "project/\n",
    "│\n",
    "├── src/\n",
    "│   ├── my_module.py\n",
    "│   └── another_module.py\n",
    "│\n",
    "└── tests/\n",
    "    ├── test_my_module.py\n",
    "    └── test_another_module.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
